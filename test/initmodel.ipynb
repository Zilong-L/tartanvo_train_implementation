{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# 添加模块搜索路径\n",
    "module_path = '/home/lzl/code/python/tartanvo_train_implementation'\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['PYTHONPATH']='/home/lzl/code/python/tartanvo_train_implementation'\n",
    "import torch\n",
    "import sys\n",
    "import toml\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from torch.utils.tensorboard import SummaryWriter   \n",
    "\n",
    "from Network.VONet import VONet\n",
    "\n",
    "from utils.train_pose_utils import load_from_file,load_model, save_model, train_pose_batch, validate\n",
    "import argparse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model, optimizer=None, scheduler=None, filepath=\"\"):\n",
    "    \"\"\"\n",
    "    加载模型、优化器和调度器的状态\n",
    "\n",
    "    参数:\n",
    "    model (torch.nn.Module): 要加载状态的模型\n",
    "    optimizer (torch.optim.Optimizer): 要加载状态的优化器\n",
    "    scheduler (torch.optim.lr_scheduler._LRScheduler): 要加载状态的调度器\n",
    "    filepath (str): 要加载文件的路径\n",
    "\n",
    "    返回:\n",
    "    int: 加载的epoch\n",
    "    int: 加载的迭代步数\n",
    "    \"\"\"\n",
    "    if filepath==\"\":\n",
    "        return 0,0\n",
    "    checkpoint = torch.load(filepath)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    if optimizer is not None:\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    if scheduler is not None:\n",
    "        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "    epoch = checkpoint['epoch']  # 如果epoch没保存，默认值为0\n",
    "    iteration = checkpoint['iteration']\n",
    "    print(f\"successfully load model from {filepath}\")\n",
    "    return epoch+1, iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = VONet()\n",
    "model = torch.nn.DataParallel(model).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwcweight = torch.load('/home/lzl/code/python/tartanvo_train_implementation/models/pwcdcnet/pwc_net_chairs.pth.tar')\n",
    "flowposeweight = torch.load('/home/lzl/code/python/tartanvo_train_implementation/models/POSEONLY_RCR/flowpose_model_iteration_27022.pth')\n",
    "strippedFlowPose = {k.replace('module.',''):v for k,v in flowposeweight['model_state_dict'].items()}\n",
    "    \n",
    "model.module.flowNet.load_state_dict(pwcweight)\n",
    "model.module.flowPoseNet.load_state_dict(strippedFlowPose)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flowNet.conv1a.0.weight torch.Size([16, 3, 3, 3])\n",
      "flowNet.conv1a.0.bias torch.Size([16])\n",
      "flowNet.conv1aa.0.weight torch.Size([16, 16, 3, 3])\n",
      "flowNet.conv1aa.0.bias torch.Size([16])\n",
      "flowNet.conv1b.0.weight torch.Size([16, 16, 3, 3])\n",
      "flowNet.conv1b.0.bias torch.Size([16])\n",
      "flowNet.conv2a.0.weight torch.Size([32, 16, 3, 3])\n",
      "flowNet.conv2a.0.bias torch.Size([32])\n",
      "flowNet.conv2aa.0.weight torch.Size([32, 32, 3, 3])\n",
      "flowNet.conv2aa.0.bias torch.Size([32])\n",
      "flowNet.conv2b.0.weight torch.Size([32, 32, 3, 3])\n",
      "flowNet.conv2b.0.bias torch.Size([32])\n",
      "flowNet.conv3a.0.weight torch.Size([64, 32, 3, 3])\n",
      "flowNet.conv3a.0.bias torch.Size([64])\n",
      "flowNet.conv3aa.0.weight torch.Size([64, 64, 3, 3])\n",
      "flowNet.conv3aa.0.bias torch.Size([64])\n",
      "flowNet.conv3b.0.weight torch.Size([64, 64, 3, 3])\n",
      "flowNet.conv3b.0.bias torch.Size([64])\n",
      "flowNet.conv4a.0.weight torch.Size([96, 64, 3, 3])\n",
      "flowNet.conv4a.0.bias torch.Size([96])\n",
      "flowNet.conv4aa.0.weight torch.Size([96, 96, 3, 3])\n",
      "flowNet.conv4aa.0.bias torch.Size([96])\n",
      "flowNet.conv4b.0.weight torch.Size([96, 96, 3, 3])\n",
      "flowNet.conv4b.0.bias torch.Size([96])\n",
      "flowNet.conv5a.0.weight torch.Size([128, 96, 3, 3])\n",
      "flowNet.conv5a.0.bias torch.Size([128])\n",
      "flowNet.conv5aa.0.weight torch.Size([128, 128, 3, 3])\n",
      "flowNet.conv5aa.0.bias torch.Size([128])\n",
      "flowNet.conv5b.0.weight torch.Size([128, 128, 3, 3])\n",
      "flowNet.conv5b.0.bias torch.Size([128])\n",
      "flowNet.conv6aa.0.weight torch.Size([196, 128, 3, 3])\n",
      "flowNet.conv6aa.0.bias torch.Size([196])\n",
      "flowNet.conv6a.0.weight torch.Size([196, 196, 3, 3])\n",
      "flowNet.conv6a.0.bias torch.Size([196])\n",
      "flowNet.conv6b.0.weight torch.Size([196, 196, 3, 3])\n",
      "flowNet.conv6b.0.bias torch.Size([196])\n",
      "flowNet.conv6_0.0.weight torch.Size([128, 81, 3, 3])\n",
      "flowNet.conv6_0.0.bias torch.Size([128])\n",
      "flowNet.conv6_1.0.weight torch.Size([128, 209, 3, 3])\n",
      "flowNet.conv6_1.0.bias torch.Size([128])\n",
      "flowNet.conv6_2.0.weight torch.Size([96, 337, 3, 3])\n",
      "flowNet.conv6_2.0.bias torch.Size([96])\n",
      "flowNet.conv6_3.0.weight torch.Size([64, 433, 3, 3])\n",
      "flowNet.conv6_3.0.bias torch.Size([64])\n",
      "flowNet.conv6_4.0.weight torch.Size([32, 497, 3, 3])\n",
      "flowNet.conv6_4.0.bias torch.Size([32])\n",
      "flowNet.predict_flow6.weight torch.Size([2, 529, 3, 3])\n",
      "flowNet.predict_flow6.bias torch.Size([2])\n",
      "flowNet.deconv6.weight torch.Size([2, 2, 4, 4])\n",
      "flowNet.deconv6.bias torch.Size([2])\n",
      "flowNet.upfeat6.weight torch.Size([529, 2, 4, 4])\n",
      "flowNet.upfeat6.bias torch.Size([2])\n",
      "flowNet.conv5_0.0.weight torch.Size([128, 213, 3, 3])\n",
      "flowNet.conv5_0.0.bias torch.Size([128])\n",
      "flowNet.conv5_1.0.weight torch.Size([128, 341, 3, 3])\n",
      "flowNet.conv5_1.0.bias torch.Size([128])\n",
      "flowNet.conv5_2.0.weight torch.Size([96, 469, 3, 3])\n",
      "flowNet.conv5_2.0.bias torch.Size([96])\n",
      "flowNet.conv5_3.0.weight torch.Size([64, 565, 3, 3])\n",
      "flowNet.conv5_3.0.bias torch.Size([64])\n",
      "flowNet.conv5_4.0.weight torch.Size([32, 629, 3, 3])\n",
      "flowNet.conv5_4.0.bias torch.Size([32])\n",
      "flowNet.predict_flow5.weight torch.Size([2, 661, 3, 3])\n",
      "flowNet.predict_flow5.bias torch.Size([2])\n",
      "flowNet.deconv5.weight torch.Size([2, 2, 4, 4])\n",
      "flowNet.deconv5.bias torch.Size([2])\n",
      "flowNet.upfeat5.weight torch.Size([661, 2, 4, 4])\n",
      "flowNet.upfeat5.bias torch.Size([2])\n",
      "flowNet.conv4_0.0.weight torch.Size([128, 181, 3, 3])\n",
      "flowNet.conv4_0.0.bias torch.Size([128])\n",
      "flowNet.conv4_1.0.weight torch.Size([128, 309, 3, 3])\n",
      "flowNet.conv4_1.0.bias torch.Size([128])\n",
      "flowNet.conv4_2.0.weight torch.Size([96, 437, 3, 3])\n",
      "flowNet.conv4_2.0.bias torch.Size([96])\n",
      "flowNet.conv4_3.0.weight torch.Size([64, 533, 3, 3])\n",
      "flowNet.conv4_3.0.bias torch.Size([64])\n",
      "flowNet.conv4_4.0.weight torch.Size([32, 597, 3, 3])\n",
      "flowNet.conv4_4.0.bias torch.Size([32])\n",
      "flowNet.predict_flow4.weight torch.Size([2, 629, 3, 3])\n",
      "flowNet.predict_flow4.bias torch.Size([2])\n",
      "flowNet.deconv4.weight torch.Size([2, 2, 4, 4])\n",
      "flowNet.deconv4.bias torch.Size([2])\n",
      "flowNet.upfeat4.weight torch.Size([629, 2, 4, 4])\n",
      "flowNet.upfeat4.bias torch.Size([2])\n",
      "flowNet.conv3_0.0.weight torch.Size([128, 149, 3, 3])\n",
      "flowNet.conv3_0.0.bias torch.Size([128])\n",
      "flowNet.conv3_1.0.weight torch.Size([128, 277, 3, 3])\n",
      "flowNet.conv3_1.0.bias torch.Size([128])\n",
      "flowNet.conv3_2.0.weight torch.Size([96, 405, 3, 3])\n",
      "flowNet.conv3_2.0.bias torch.Size([96])\n",
      "flowNet.conv3_3.0.weight torch.Size([64, 501, 3, 3])\n",
      "flowNet.conv3_3.0.bias torch.Size([64])\n",
      "flowNet.conv3_4.0.weight torch.Size([32, 565, 3, 3])\n",
      "flowNet.conv3_4.0.bias torch.Size([32])\n",
      "flowNet.predict_flow3.weight torch.Size([2, 597, 3, 3])\n",
      "flowNet.predict_flow3.bias torch.Size([2])\n",
      "flowNet.deconv3.weight torch.Size([2, 2, 4, 4])\n",
      "flowNet.deconv3.bias torch.Size([2])\n",
      "flowNet.upfeat3.weight torch.Size([597, 2, 4, 4])\n",
      "flowNet.upfeat3.bias torch.Size([2])\n",
      "flowNet.conv2_0.0.weight torch.Size([128, 117, 3, 3])\n",
      "flowNet.conv2_0.0.bias torch.Size([128])\n",
      "flowNet.conv2_1.0.weight torch.Size([128, 245, 3, 3])\n",
      "flowNet.conv2_1.0.bias torch.Size([128])\n",
      "flowNet.conv2_2.0.weight torch.Size([96, 373, 3, 3])\n",
      "flowNet.conv2_2.0.bias torch.Size([96])\n",
      "flowNet.conv2_3.0.weight torch.Size([64, 469, 3, 3])\n",
      "flowNet.conv2_3.0.bias torch.Size([64])\n",
      "flowNet.conv2_4.0.weight torch.Size([32, 533, 3, 3])\n",
      "flowNet.conv2_4.0.bias torch.Size([32])\n",
      "flowNet.predict_flow2.weight torch.Size([2, 565, 3, 3])\n",
      "flowNet.predict_flow2.bias torch.Size([2])\n",
      "flowNet.deconv2.weight torch.Size([2, 2, 4, 4])\n",
      "flowNet.deconv2.bias torch.Size([2])\n",
      "flowNet.dc_conv1.0.weight torch.Size([128, 565, 3, 3])\n",
      "flowNet.dc_conv1.0.bias torch.Size([128])\n",
      "flowNet.dc_conv2.0.weight torch.Size([128, 128, 3, 3])\n",
      "flowNet.dc_conv2.0.bias torch.Size([128])\n",
      "flowNet.dc_conv3.0.weight torch.Size([128, 128, 3, 3])\n",
      "flowNet.dc_conv3.0.bias torch.Size([128])\n",
      "flowNet.dc_conv4.0.weight torch.Size([96, 128, 3, 3])\n",
      "flowNet.dc_conv4.0.bias torch.Size([96])\n",
      "flowNet.dc_conv5.0.weight torch.Size([64, 96, 3, 3])\n",
      "flowNet.dc_conv5.0.bias torch.Size([64])\n",
      "flowNet.dc_conv6.0.weight torch.Size([32, 64, 3, 3])\n",
      "flowNet.dc_conv6.0.bias torch.Size([32])\n",
      "flowNet.dc_conv7.weight torch.Size([2, 32, 3, 3])\n",
      "flowNet.dc_conv7.bias torch.Size([2])\n",
      "flowPoseNet.firstconv.0.0.weight torch.Size([32, 4, 3, 3])\n",
      "flowPoseNet.firstconv.0.0.bias torch.Size([32])\n",
      "flowPoseNet.firstconv.1.0.weight torch.Size([32, 32, 3, 3])\n",
      "flowPoseNet.firstconv.1.0.bias torch.Size([32])\n",
      "flowPoseNet.firstconv.2.0.weight torch.Size([32, 32, 3, 3])\n",
      "flowPoseNet.firstconv.2.0.bias torch.Size([32])\n",
      "flowPoseNet.layer1.0.conv1.0.weight torch.Size([64, 32, 3, 3])\n",
      "flowPoseNet.layer1.0.conv1.0.bias torch.Size([64])\n",
      "flowPoseNet.layer1.0.conv2.weight torch.Size([64, 64, 3, 3])\n",
      "flowPoseNet.layer1.0.conv2.bias torch.Size([64])\n",
      "flowPoseNet.layer1.0.downsample.weight torch.Size([64, 32, 1, 1])\n",
      "flowPoseNet.layer1.0.downsample.bias torch.Size([64])\n",
      "flowPoseNet.layer1.1.conv1.0.weight torch.Size([64, 64, 3, 3])\n",
      "flowPoseNet.layer1.1.conv1.0.bias torch.Size([64])\n",
      "flowPoseNet.layer1.1.conv2.weight torch.Size([64, 64, 3, 3])\n",
      "flowPoseNet.layer1.1.conv2.bias torch.Size([64])\n",
      "flowPoseNet.layer1.2.conv1.0.weight torch.Size([64, 64, 3, 3])\n",
      "flowPoseNet.layer1.2.conv1.0.bias torch.Size([64])\n",
      "flowPoseNet.layer1.2.conv2.weight torch.Size([64, 64, 3, 3])\n",
      "flowPoseNet.layer1.2.conv2.bias torch.Size([64])\n",
      "flowPoseNet.layer2.0.conv1.0.weight torch.Size([128, 64, 3, 3])\n",
      "flowPoseNet.layer2.0.conv1.0.bias torch.Size([128])\n",
      "flowPoseNet.layer2.0.conv2.weight torch.Size([128, 128, 3, 3])\n",
      "flowPoseNet.layer2.0.conv2.bias torch.Size([128])\n",
      "flowPoseNet.layer2.0.downsample.weight torch.Size([128, 64, 1, 1])\n",
      "flowPoseNet.layer2.0.downsample.bias torch.Size([128])\n",
      "flowPoseNet.layer2.1.conv1.0.weight torch.Size([128, 128, 3, 3])\n",
      "flowPoseNet.layer2.1.conv1.0.bias torch.Size([128])\n",
      "flowPoseNet.layer2.1.conv2.weight torch.Size([128, 128, 3, 3])\n",
      "flowPoseNet.layer2.1.conv2.bias torch.Size([128])\n",
      "flowPoseNet.layer2.2.conv1.0.weight torch.Size([128, 128, 3, 3])\n",
      "flowPoseNet.layer2.2.conv1.0.bias torch.Size([128])\n",
      "flowPoseNet.layer2.2.conv2.weight torch.Size([128, 128, 3, 3])\n",
      "flowPoseNet.layer2.2.conv2.bias torch.Size([128])\n",
      "flowPoseNet.layer2.3.conv1.0.weight torch.Size([128, 128, 3, 3])\n",
      "flowPoseNet.layer2.3.conv1.0.bias torch.Size([128])\n",
      "flowPoseNet.layer2.3.conv2.weight torch.Size([128, 128, 3, 3])\n",
      "flowPoseNet.layer2.3.conv2.bias torch.Size([128])\n",
      "flowPoseNet.layer3.0.conv1.0.weight torch.Size([128, 128, 3, 3])\n",
      "flowPoseNet.layer3.0.conv1.0.bias torch.Size([128])\n",
      "flowPoseNet.layer3.0.conv2.weight torch.Size([128, 128, 3, 3])\n",
      "flowPoseNet.layer3.0.conv2.bias torch.Size([128])\n",
      "flowPoseNet.layer3.0.downsample.weight torch.Size([128, 128, 1, 1])\n",
      "flowPoseNet.layer3.0.downsample.bias torch.Size([128])\n",
      "flowPoseNet.layer3.1.conv1.0.weight torch.Size([128, 128, 3, 3])\n",
      "flowPoseNet.layer3.1.conv1.0.bias torch.Size([128])\n",
      "flowPoseNet.layer3.1.conv2.weight torch.Size([128, 128, 3, 3])\n",
      "flowPoseNet.layer3.1.conv2.bias torch.Size([128])\n",
      "flowPoseNet.layer3.2.conv1.0.weight torch.Size([128, 128, 3, 3])\n",
      "flowPoseNet.layer3.2.conv1.0.bias torch.Size([128])\n",
      "flowPoseNet.layer3.2.conv2.weight torch.Size([128, 128, 3, 3])\n",
      "flowPoseNet.layer3.2.conv2.bias torch.Size([128])\n",
      "flowPoseNet.layer3.3.conv1.0.weight torch.Size([128, 128, 3, 3])\n",
      "flowPoseNet.layer3.3.conv1.0.bias torch.Size([128])\n",
      "flowPoseNet.layer3.3.conv2.weight torch.Size([128, 128, 3, 3])\n",
      "flowPoseNet.layer3.3.conv2.bias torch.Size([128])\n",
      "flowPoseNet.layer3.4.conv1.0.weight torch.Size([128, 128, 3, 3])\n",
      "flowPoseNet.layer3.4.conv1.0.bias torch.Size([128])\n",
      "flowPoseNet.layer3.4.conv2.weight torch.Size([128, 128, 3, 3])\n",
      "flowPoseNet.layer3.4.conv2.bias torch.Size([128])\n",
      "flowPoseNet.layer3.5.conv1.0.weight torch.Size([128, 128, 3, 3])\n",
      "flowPoseNet.layer3.5.conv1.0.bias torch.Size([128])\n",
      "flowPoseNet.layer3.5.conv2.weight torch.Size([128, 128, 3, 3])\n",
      "flowPoseNet.layer3.5.conv2.bias torch.Size([128])\n",
      "flowPoseNet.layer4.0.conv1.0.weight torch.Size([256, 128, 3, 3])\n",
      "flowPoseNet.layer4.0.conv1.0.bias torch.Size([256])\n",
      "flowPoseNet.layer4.0.conv2.weight torch.Size([256, 256, 3, 3])\n",
      "flowPoseNet.layer4.0.conv2.bias torch.Size([256])\n",
      "flowPoseNet.layer4.0.downsample.weight torch.Size([256, 128, 1, 1])\n",
      "flowPoseNet.layer4.0.downsample.bias torch.Size([256])\n",
      "flowPoseNet.layer4.1.conv1.0.weight torch.Size([256, 256, 3, 3])\n",
      "flowPoseNet.layer4.1.conv1.0.bias torch.Size([256])\n",
      "flowPoseNet.layer4.1.conv2.weight torch.Size([256, 256, 3, 3])\n",
      "flowPoseNet.layer4.1.conv2.bias torch.Size([256])\n",
      "flowPoseNet.layer4.2.conv1.0.weight torch.Size([256, 256, 3, 3])\n",
      "flowPoseNet.layer4.2.conv1.0.bias torch.Size([256])\n",
      "flowPoseNet.layer4.2.conv2.weight torch.Size([256, 256, 3, 3])\n",
      "flowPoseNet.layer4.2.conv2.bias torch.Size([256])\n",
      "flowPoseNet.layer4.3.conv1.0.weight torch.Size([256, 256, 3, 3])\n",
      "flowPoseNet.layer4.3.conv1.0.bias torch.Size([256])\n",
      "flowPoseNet.layer4.3.conv2.weight torch.Size([256, 256, 3, 3])\n",
      "flowPoseNet.layer4.3.conv2.bias torch.Size([256])\n",
      "flowPoseNet.layer4.4.conv1.0.weight torch.Size([256, 256, 3, 3])\n",
      "flowPoseNet.layer4.4.conv1.0.bias torch.Size([256])\n",
      "flowPoseNet.layer4.4.conv2.weight torch.Size([256, 256, 3, 3])\n",
      "flowPoseNet.layer4.4.conv2.bias torch.Size([256])\n",
      "flowPoseNet.layer4.5.conv1.0.weight torch.Size([256, 256, 3, 3])\n",
      "flowPoseNet.layer4.5.conv1.0.bias torch.Size([256])\n",
      "flowPoseNet.layer4.5.conv2.weight torch.Size([256, 256, 3, 3])\n",
      "flowPoseNet.layer4.5.conv2.bias torch.Size([256])\n",
      "flowPoseNet.layer4.6.conv1.0.weight torch.Size([256, 256, 3, 3])\n",
      "flowPoseNet.layer4.6.conv1.0.bias torch.Size([256])\n",
      "flowPoseNet.layer4.6.conv2.weight torch.Size([256, 256, 3, 3])\n",
      "flowPoseNet.layer4.6.conv2.bias torch.Size([256])\n",
      "flowPoseNet.layer5.0.conv1.0.weight torch.Size([256, 256, 3, 3])\n",
      "flowPoseNet.layer5.0.conv1.0.bias torch.Size([256])\n",
      "flowPoseNet.layer5.0.conv2.weight torch.Size([256, 256, 3, 3])\n",
      "flowPoseNet.layer5.0.conv2.bias torch.Size([256])\n",
      "flowPoseNet.layer5.0.downsample.weight torch.Size([256, 256, 1, 1])\n",
      "flowPoseNet.layer5.0.downsample.bias torch.Size([256])\n",
      "flowPoseNet.layer5.1.conv1.0.weight torch.Size([256, 256, 3, 3])\n",
      "flowPoseNet.layer5.1.conv1.0.bias torch.Size([256])\n",
      "flowPoseNet.layer5.1.conv2.weight torch.Size([256, 256, 3, 3])\n",
      "flowPoseNet.layer5.1.conv2.bias torch.Size([256])\n",
      "flowPoseNet.layer5.2.conv1.0.weight torch.Size([256, 256, 3, 3])\n",
      "flowPoseNet.layer5.2.conv1.0.bias torch.Size([256])\n",
      "flowPoseNet.layer5.2.conv2.weight torch.Size([256, 256, 3, 3])\n",
      "flowPoseNet.layer5.2.conv2.bias torch.Size([256])\n",
      "flowPoseNet.voflow_trans.0.0.weight torch.Size([128, 1536])\n",
      "flowPoseNet.voflow_trans.0.0.bias torch.Size([128])\n",
      "flowPoseNet.voflow_trans.1.0.weight torch.Size([32, 128])\n",
      "flowPoseNet.voflow_trans.1.0.bias torch.Size([32])\n",
      "flowPoseNet.voflow_trans.2.weight torch.Size([3, 32])\n",
      "flowPoseNet.voflow_trans.2.bias torch.Size([3])\n",
      "flowPoseNet.voflow_rot.0.0.weight torch.Size([128, 1536])\n",
      "flowPoseNet.voflow_rot.0.0.bias torch.Size([128])\n",
      "flowPoseNet.voflow_rot.1.0.weight torch.Size([32, 128])\n",
      "flowPoseNet.voflow_rot.1.0.bias torch.Size([32])\n",
      "flowPoseNet.voflow_rot.2.weight torch.Size([3, 32])\n",
      "flowPoseNet.voflow_rot.2.bias torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "for k,v in model.module.state_dict().items():\n",
    "    print(k,v.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block after 'for' statement on line 5 (654167195.py, line 10)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[37], line 10\u001b[0;36m\u001b[0m\n\u001b[0;31m    dataloader_dict = {}\u001b[0m\n\u001b[0m                        ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block after 'for' statement on line 5\n"
     ]
    }
   ],
   "source": [
    "# 循环中加载数据集\n",
    "for epoch in num_epoch:\n",
    "    dataset = load_from_file('path_to_dataset')\n",
    "    dataloader = create_dataloader(dataset)\n",
    "    for sample in dataloader:\n",
    "        # tarin code\n",
    "\n",
    "\n",
    "# 一开始加载所有数据集\n",
    "dataloader_dict = {}\n",
    "for path in all_dataset_path:\n",
    "    dataset = load_from_file(path)\n",
    "    dataloader = create_dataloader(dataset)\n",
    "    dataloader_dict[path] = dataloader\n",
    "\n",
    "for epoch in num_epoch:\n",
    "    for path, dataloader in dataloader_dict.items():\n",
    "        for sample in dataloader:\n",
    "            # tarin code\n",
    "    \n",
    "\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tartan",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
